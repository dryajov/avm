12/06/2016
==========

1. What will happen if we spin_unlock a unlocked lock.
2. Which is faster, taking a unacquired spinlock or using atomic test_and_set_bit()
3. skiplist is good when we dont have access to lba range array. 
4. Its better to use lba_lock with a single bit
5. Normal size of lba_block = 10 bytes, pba = 15 and hash = 5 bytes
//LBA Block main data structure
typedef struct  lba_block_s {
	CHAR 	lock:1;
	BOOL	allocated;
	U32		hash_index:31;
	U64 	hash_collision_index:33; 	//Used in case of dedupe or hash collision. 
	// Basically it is an index into  multiple pba_entries which are linked during hash collision
}__attribute__((__packed__)) LBA_BLOCK, *PLBA_BLOCK;

//HASH Block main structure
typedef struct hash_block_s {
	U64 pba_index:33;
}__attribute__((__packed__)) HASH_BLOCK, *PHASH_BLOCK; 

//PBA Block main structure
typedef struct pba_block_s {
	U64 hash_collision_index:33;
	U32 size:13;
	U64 ref_cnt:34;
	U64 next_hash_collision_index:33; // Index of Next PBA in a collision list
}__attribute__((__packed__)) PBA_BLOCK, *PPBA_BLOCK; 

6. When does typecasting happens in c. Compile time or runtime
7. Can we avoid fetching free pba inside LBA, HBA and PBA lock. What if we proactively
fetch a free pba before taking any LBA LOCK i.e. just before going into the write or
overwrite process.
A: We can do that proactively but in case of dedupes no free pba is required. In that case
proactive activity is a waste. Also disk IOs are issued based on the pba_index because that 
acts as the actual sector on which the pba is going to be written. And that is how, we are 
achieving random to sequential conversion. Proactive fetching means
the actualy sequence of disk writes will not be maintainted because for dedupe writes, no 
disk write will be required and that might break the sequence on the disk.
We can still achieve sequential write by not taking pba_index as next disk write sector.
We can allocate a counter globally which will be updated when the actual disk write happens.
But a lock will be required to protect that counter. Looks like it is doable

8. How to atomically increment U64 integer. The current kernel functions defines atomic_64 as
long 
typedef struct {
   long counter;
} atomic64_t;
Can we just use the source code of atomic64_add and pass our U64  instead of long pointer
static __inline__ void atomic64_add(long i, atomic64_t * v)
{
    unsigned long temp;
    __asm__ __volatile__(
    "1: ldq_l %0,%1\n"
    "   addq %0,%2,%0\n"
    "   stq_c %0,%1\n"
    "   beq %0,2f\n"
    ".subsection 2\n"
    "2: br 1b\n"
    ".previous"
    :"=&r" (temp), "=m" (v->counter)
    :"Ir" (i), "m" (v->counter));
}

9. Currently we are using bit_spin_lock for writing into the LBA. Should we use this lock
for reading as well.

10. dz_disk_io_sector is assigned without lock. We should protect this with a atomic variable or 
a spin lock. Also handle the overlapp scenario. But this is good for the first time i.e. an AISA
device is created freshly. What will happen to reboot after data is written to the disk. In that
case this will not work

11. How can we tune device-mapper to always create bio with page size vectors only i.e.
bv_offset is always zero and bv_len is always 4k except for the partial IO.
Also partial IO should always be having vector count as 1 only.

12. We need to tune the IO Scheduler for our disk writes. The default one may not work.
We need to use that scheduler which favours IOs on SSDs/Nand devices and RAM/NVDIMM

13. How can we stop device-mapper to issue multiple reads just after creating the device
using dmsetup create command

14. We need to have a AEN culture where any sort of failure encountered by lower layers should be intimated
to AISA driver immediately.
E.g. we are opening metadata device /dev/md0 during aisa_ctr. If there is a failure of this device
in silence then mdraid should notify us immediately and not at the time when IO is issued on that
device.

15. Can we pass the metadata device name during dmsetup. Will it have any advantage
Yes, this will help to provide the UUID which is calculated in user space
Also this allows more flexibility for choosing which device for metadata

16. Can we club data and metadata together in one 4K block. 
Right now if we have seperate disks for data and metadata  then we are facing these problems:
1. Writing metadata which is typically in 8 bytes means we have to read 512 bytes from the mdd
and merge the changes and then write to the mdd.
2. This means one application write involves 1 data write of 4K plus 1 read of 512 from MDD
and then 1 512 write to MDD.
3. This means one application write involves 2 writes and 1 read.
There is one paper which talks about this issue:
http://www.ece.eng.wayne.edu/~sjiang/pubs/papers/wu15-selfie.pdf
4. First of all keeping data and metadata on different disk is a good idea or not?
IMO, yes because that means user will get the same amount of storage what is promised during
lun creation.
NOrmally vendors write data and metadata together on the disk and that reduces the 
overall capacity of the storage. This is ethically wrong

For the time being, we can solve this problem by readback the 512 bytes, merge the changes
and then write. Another problem in this approach is that, we have to lock the entire 512 bytes.
so if there are other writes waiting to happen in the next metadata which also resides in same
512 bytes then they will have to wait.
For sequential writes, this is a problem, but since we are doing in the background, then
application IO will not be impacted. Because we might be usin write-back cache with the help
of NVDIMM or battery backedup ram

17. Now I can think of two product lines for our Flash Array
1. Where MetaData is on SSD and DATA is on HDD. It is called as Hybrid Array.
2. Where both are on SSD. It is called as All Flash Array
FrontEnd Writeback caching is provided using NVDIMM cards
BackEnd caching is provided using SSDs.

In #1, there is no need to have Garbage colletion
In #2, we need the GC for Data Disks because there we'll be writing
sequentially with inline dedupe

18.  We can have two forms of inline deduplication
1. Inline-Async : Here Write IO will be stored directly in frontend cache (NVDIMM cache)
and acknowledged to application that write is finished. Here calculation of hash will be done
asynchronously.
2. Inline-Sync : Here Write IO will be store to frontend cache but acknowledged when its hash
is calculated

9/07/2016:
==========
1. We need to handle the child failed scenarios i.e. how to convey the child errors
to parent bio during done function. Do we need to do parent biodone when a child 
encounters error?

18/07/2016
Q:How expensive is it to dereference a pointer in C?
1.Dereferencing can be expensive mostly because it costs an instruction to
fetch data from memory which might be far away and do not exhibit 
locality of reference. In that case, the processor should fetch data 
from non-cached memory and even hard disk (in case of a hard page fault).

2. The most important factor in dereferencing pointers on ordinary 
systems is that you're likely to generate a cache miss. A random 
access in SDRAM memory costs tens of nanoseconds (e.g. 64). On 
gigaherz processors, this means that your processor is idling 
hundreds (or > thousand) of cycles, without being able of doing 
anything else in the meantime.

Only on SRAM based systems (which you'll only find in embedded software), 
or when your software is cache optimized, the factors discussed in the 
other posts come into play.

28_07_2016
drop_caches
-----------

Writing to this will cause the kernel to drop clean caches, dentries and
inodes from memory, causing that memory to become free.

To free pagecache:
	echo 1 > /proc/sys/vm/drop_caches
To free dentries and inodes:
	echo 2 > /proc/sys/vm/drop_caches
To free pagecache, dentries and inodes:
	echo 3 > /proc/sys/vm/drop_caches

01_08_2016:
-----------
Make a sleep/wakeup for bit based locks
enum pageflags {
	PG_locked,      /* Page is locked. Don't touch. */
	PG_error,
...
}
static inline int trylock_page(struct page *page)
{
	return (likely(!test_and_set_bit_lock(PG_locked, &page->flags)));
}

int __sched
__wait_on_bit_lock(wait_queue_head_t *wq, struct wait_bit_queue *q,
            int (*action)(void *), unsigned mode)
{
    do {
        int ret;

        prepare_to_wait_exclusive(wq, &q->wait, mode);
        if (!test_bit(q->key.bit_nr, q->key.flags))
            continue;
        ret = action(q->key.flags);
        if (!ret)
            continue;
        abort_exclusive_wait(wq, &q->wait, mode, &q->key);
        return ret;
    } while (test_and_set_bit(q->key.bit_nr, q->key.flags));
    finish_wait(wq, &q->wait);
    return 0;
}


void __lock_page(struct page *page)
{
    DEFINE_WAIT_BIT(wait, &page->flags, PG_locked);

    __wait_on_bit_lock(page_waitqueue(page), &wait, sleep_on_page,
                            TASK_UNINTERRUPTIBLE);
}

static inline void lock_page(struct page *page)
{
	might_sleep();
	if (!trylock_page(page))
	__lock_page(page);
}

